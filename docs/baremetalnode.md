# Baremetal Node setup for Training Cluster

## Node installation

https://docs.hetzner.com/de/robot/dedicated-server/operating-systems/installimage

Ubuntu 22.04

Software Raid 1 and LVM with one volume group (vg0), one physical volume for /dev/md1 and  3 logical volumes:
* /dev/vg0/root -> mounted at /
* /dev/vg0/varlib ->  mounted at /var/lib/
* /dev/vg0/mnt -> mounted at /mnt (for e.g. longhorn)
* /boot is a ext3 /this cant be a lv. this is /dev/md0

check examples in installimage config on how to do this (I don't remember the exact config anymore...)

## Network

Create vSwitch and connect to Cloud in https://robot.hetzner.com/ and https://console.hetzner.cloud/
See also https://docs.hetzner.com/de/cloud/networks/connect-dedi-vswitch/

Example, make sure to not use the same IP twice

```yaml
  vlans:
    enp4s0.4000:
      id: 4000
      link: enp4s0
      mtu: 1400
      addresses:
        - 10.0.1.2/24
      routes:
        - to: "10.0.0.0/8"
          via: "10.0.1.1"
```

Verify connectivy e.g. `curl -k  https://10.0.0.x:6443` (ip of k8s lb)

Make sure cloud firewall has the ip's of the baremetal node listed in the firewall rule for rke2 api (tcp/9345) (check network.tf)

## Node preparation

```bash
apt install jq curl git open-iscsi nfs-common
echo "*               soft    nofile          1000000" >> /etc/security/limits.conf
echo "*               hard    nofile          1000000" >> /etc/security/limits.conf
echo 'fs.file-max = 1000000' >> /etc/sysctl.conf
echo 'fs.inotify.max_user_instances=8192' >> /etc/sysctl.conf
echo 'fs.inotify.max_user_watches=524288' >> /etc/sysctl.conf
```

## Installation of rke2

```bash
chmod u+x install.sh
INSTALL_RKE2_METHOD='tar' INSTALL_RKE2_TYPE=agent INSTALL_RKE2_VERSION=v1.30.1+rke2r1 ./install.sh
curl -fsSL https://raw.githubusercontent.com/rancher/rke2/master/install.sh --output install.sh
```

RKE2 Config: /etc/rancher/rke2/config.yaml

```yaml
server: https://10.0.0.x:9345 # rke2 lb
token: xxx 
#cloud-provider-name: external
node-ip: 10.0.1.x
kubelet-arg:
 - "provider-id=baremetal://custom01" # this is a hack to prevent the hetzner ccm to remove the node. but tbv, with overlay setup this might not be needed anymore and ccm can also handle robot server in this mode (not tested yet, but cluster uses overlay now instead of native routed mode)
 ```

Containerd Config /var/lib/rancher/rke2/agent/etc/containerd/config.toml.tmpl:

```toml
{{- /* */ -}}
# File generated by {{ .Program }}. DO NOT EDIT. Use config.toml.tmpl instead.
# changes for kubevirt
version = 2

[plugins."io.containerd.internal.v1.opt"]
  path = "{{ .NodeConfig.Containerd.Opt }}"
[plugins."io.containerd.grpc.v1.cri"]
  stream_server_address = "127.0.0.1"
  stream_server_port = "10010"
  enable_selinux = {{ .NodeConfig.SELinux }}
  enable_unprivileged_ports = {{ .EnableUnprivileged }}
  enable_unprivileged_icmp = {{ .EnableUnprivileged }}
  device_ownership_from_security_context = true

{{- if .DisableCgroup}}
  disable_cgroup = true
{{end}}
{{- if .IsRunningInUserNS }}
  disable_apparmor = true
  restrict_oom_score_adj = true
{{end}}

{{- if .NodeConfig.AgentConfig.PauseImage }}
  sandbox_image = "{{ .NodeConfig.AgentConfig.PauseImage }}"
{{end}}

{{- if .NodeConfig.AgentConfig.Snapshotter }}
[plugins."io.containerd.grpc.v1.cri".containerd]
  snapshotter = "{{ .NodeConfig.AgentConfig.Snapshotter }}"
  disable_snapshot_annotations = {{ if eq .NodeConfig.AgentConfig.Snapshotter "stargz" }}false{{else}}true{{end}}
  {{ if .NodeConfig.DefaultRuntime }}default_runtime_name = "{{ .NodeConfig.DefaultRuntime }}"{{end}}
{{ if eq .NodeConfig.AgentConfig.Snapshotter "stargz" }}
{{ if .NodeConfig.AgentConfig.ImageServiceSocket }}
[plugins."io.containerd.snapshotter.v1.stargz"]
cri_keychain_image_service_path = "{{ .NodeConfig.AgentConfig.ImageServiceSocket }}"
[plugins."io.containerd.snapshotter.v1.stargz".cri_keychain]
enable_keychain = true
{{end}}

[plugins."io.containerd.snapshotter.v1.stargz".registry]
  config_path = "{{ .NodeConfig.Containerd.Registry }}"

{{ if .PrivateRegistryConfig }}
{{range $k, $v := .PrivateRegistryConfig.Configs }}
{{ if $v.Auth }}
[plugins."io.containerd.snapshotter.v1.stargz".registry.configs."{{$k}}".auth]
  {{ if $v.Auth.Username }}username = {{ printf "%q" $v.Auth.Username }}{{end}}
  {{ if $v.Auth.Password }}password = {{ printf "%q" $v.Auth.Password }}{{end}}
  {{ if $v.Auth.Auth }}auth = {{ printf "%q" $v.Auth.Auth }}{{end}}
  {{ if $v.Auth.IdentityToken }}identitytoken = {{ printf "%q" $v.Auth.IdentityToken }}{{end}}
{{end}}
{{end}}
{{end}}
{{end}}
{{end}}

{{- if not .NodeConfig.NoFlannel }}
[plugins."io.containerd.grpc.v1.cri".cni]
  bin_dir = "{{ .NodeConfig.AgentConfig.CNIBinDir }}"
  conf_dir = "{{ .NodeConfig.AgentConfig.CNIConfDir }}"
{{end}}

{{- if or .NodeConfig.Containerd.BlockIOConfig .NodeConfig.Containerd.RDTConfig }}
[plugins."io.containerd.service.v1.tasks-service"]
  {{ if .NodeConfig.Containerd.BlockIOConfig }}blockio_config_file = "{{ .NodeConfig.Containerd.BlockIOConfig }}"{{end}}
  {{ if .NodeConfig.Containerd.RDTConfig }}rdt_config_file = "{{ .NodeConfig.Containerd.RDTConfig }}"{{end}}
{{end}}

[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
  runtime_type = "io.containerd.runc.v2"

[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
  SystemdCgroup = {{ .SystemdCgroup }}

[plugins."io.containerd.grpc.v1.cri".registry]
  config_path = "{{ .NodeConfig.Containerd.Registry }}"

{{ if .PrivateRegistryConfig }}
{{range $k, $v := .PrivateRegistryConfig.Configs }}
{{ if $v.Auth }}
[plugins."io.containerd.grpc.v1.cri".registry.configs."{{$k}}".auth]
  {{ if $v.Auth.Username }}username = {{ printf "%q" $v.Auth.Username }}{{end}}
  {{ if $v.Auth.Password }}password = {{ printf "%q" $v.Auth.Password }}{{end}}
  {{ if $v.Auth.Auth }}auth = {{ printf "%q" $v.Auth.Auth }}{{end}}
  {{ if $v.Auth.IdentityToken }}identitytoken = {{ printf "%q" $v.Auth.IdentityToken }}{{end}}
{{end}}
{{end}}
{{end}}

{{range $k, $v := .ExtraRuntimes}}
[plugins."io.containerd.grpc.v1.cri".containerd.runtimes."{{$k}}"]
  runtime_type = "{{$v.RuntimeType}}"
[plugins."io.containerd.grpc.v1.cri".containerd.runtimes."{{$k}}".options]
  BinaryName = "{{$v.BinaryName}}"
  SystemdCgroup = {{ $.SystemdCgroup }}
{{end}}
```

## start rke2 after config files

```bash
systemctl enable rke2-agent
systemctl start rke2-agent
```